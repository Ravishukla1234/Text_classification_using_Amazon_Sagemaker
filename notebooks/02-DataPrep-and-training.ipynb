{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "102932e5",
   "metadata": {},
   "source": [
    "# Data Preparation and Training\n",
    "\n",
    "In this notebook, we are going to prepare data and run training on Amazon Sagemaker."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea9be8a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cdd6d4",
   "metadata": {},
   "source": [
    "## Load required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1cda53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "\n",
    "import sagemaker\n",
    "import boto3\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "from sagemaker.s3 import S3Downloader\n",
    "from sagemaker.s3 import S3Uploader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_dir = \"../input\"\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "region = sagemaker_session.boto_region_name\n",
    "role = get_execution_role()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73228648",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5c6ed8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(f\"{data_dir}/train.tsv\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8edd66a",
   "metadata": {},
   "source": [
    "## Prepare the data and Upload it to s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f239f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://sagemaker-ap-south-1-296512243111/stumbleUpon/train-pkl_data\n"
     ]
    }
   ],
   "source": [
    "def extract_title_and_body(data):\n",
    "    boilerplatedf = data[\"boilerplate\"].apply(json.loads)\n",
    "    boilerplatedf = pd.DataFrame(boilerplatedf.tolist())\n",
    "    data[\"boilerplate_title\"] = boilerplatedf[\"title\"].copy()\n",
    "    data[\"boilerplate_body\"] = boilerplatedf[\"body\"].copy()   \n",
    "    data[\"boilerplate_title\"] = data[\"boilerplate_title\"].fillna('')\n",
    "    data[\"boilerplate_body\"] = data[\"boilerplate_body\"].fillna('')\n",
    "    data = data.drop(columns = [\"boilerplate\"])\n",
    "    del boilerplatedf\n",
    "    return data\n",
    "train = extract_title_and_body(train)\n",
    "train[\"boilerplate_title\"] = train[\"boilerplate_title\"].fillna(\"\")\n",
    "train[\"boilerplate_body\"] = train[\"boilerplate_body\"].fillna(\"\")\n",
    "train[\"text\"] = train[\"boilerplate_title\"] +\". \" + train[\"boilerplate_body\"]\n",
    "train[\"text\"] = train[\"text\"].str.lower()\n",
    "\n",
    "text_features = [\"text\"]\n",
    "\n",
    "xtrain =   train[text_features + [\"label\"]]\n",
    "\n",
    "x_train , x_valid = train_test_split(xtrain, test_size=0.3,random_state=2020)\n",
    "x_test , x_valid = train_test_split(x_valid, test_size=0.5,random_state=2020)\n",
    "\n",
    "y_train , x_train =  x_train[\"label\"].values, x_train[\"text\"].values\n",
    "y_valid , x_valid =  x_valid[\"label\"].values, x_valid[\"text\"].values\n",
    "y_test , x_test =  x_test[\"label\"].values, x_test[\"text\"].values\n",
    "\n",
    "# Save the dataset as pickle file\n",
    "\n",
    "pklfile = f'{data_dir}/train_data_pre_processed.pkl'\n",
    "with open(pklfile,'wb') as f:\n",
    "    pickle.dump({\n",
    "        'x_train': x_train,\n",
    "        'x_valid': x_valid,\n",
    "        'x_test' : x_test,\n",
    "        'y_train': y_train,\n",
    "        'y_valid': y_valid,\n",
    "        'y_test' : y_test\n",
    "    }, f)\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "prefix = \"stumbleUpon\"\n",
    "\n",
    "\n",
    "inputs = S3Uploader.upload(pklfile, \"s3://{}/{}/train-pkl_data\".format(bucket, prefix))\n",
    "\n",
    "inputs =  \"s3://{}/{}/train-pkl_data\".format(bucket, prefix)\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8383c2e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Train Bert model using Amazon Sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67d622f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters={\n",
    "                 \n",
    "    \"model_name\": \"bert-base-uncased\",\n",
    "    \"batch_size\": 8,\n",
    "    \"epochs\": 2 ,\n",
    "    \"lr\" : 2e-5,\n",
    "                 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "695ec61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-11-27 11:25:46 Starting - Starting the training job...\n",
      "2022-11-27 11:26:11 Starting - Preparing the instances for trainingProfilerReport-1669548346: InProgress\n",
      ".....................\n",
      "2022-11-27 11:29:35 Downloading - Downloading input data\n",
      "2022-11-27 11:29:35 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-11-27 11:29:37,220 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-11-27 11:29:37,266 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-11-27 11:29:37,268 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-11-27 11:29:37,540 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"training\": \"/opt/ml/input/data/training\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"batch_size\": 8,\n",
      "        \"epochs\": 2,\n",
      "        \"lr\": 2e-05,\n",
      "        \"model_name\": \"bert-base-uncased\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"training\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-11-27-11-25-45-726\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-south-1-296512243111/huggingface-pytorch-training-2022-11-27-11-25-45-726/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 48,\n",
      "    \"num_gpus\": 4,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.12xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.12xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"batch_size\":8,\"epochs\":2,\"lr\":2e-05,\"model_name\":\"bert-base-uncased\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"training\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=48\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=4\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-south-1-296512243111/huggingface-pytorch-training-2022-11-27-11-25-45-726/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"training\":\"/opt/ml/input/data/training\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"batch_size\":8,\"epochs\":2,\"lr\":2e-05,\"model_name\":\"bert-base-uncased\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"training\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-11-27-11-25-45-726\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-south-1-296512243111/huggingface-pytorch-training-2022-11-27-11-25-45-726/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":48,\"num_gpus\":4,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.12xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.12xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--batch_size\",\"8\",\"--epochs\",\"2\",\"--lr\",\"2e-05\",\"--model_name\",\"bert-base-uncased\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAINING=/opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mSM_HP_BATCH_SIZE=8\u001b[0m\n",
      "\u001b[34mSM_HP_EPOCHS=2\u001b[0m\n",
      "\u001b[34mSM_HP_LR=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME=bert-base-uncased\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 train.py --batch_size 8 --epochs 2 --lr 2e-05 --model_name bert-base-uncased\u001b[0m\n",
      "\u001b[34mModel will be saved in - /opt/ml/model\u001b[0m\n",
      "\u001b[34mPath to data folder - /opt/ml/input/data/training\u001b[0m\n",
      "\u001b[34mContents of folder /opt/ml/input/data/training - ['train_data_pre_processed.pkl']\u001b[0m\n",
      "\u001b[34m/opt/ml/input/data/training/train_data_pre_processed.pkl\u001b[0m\n",
      "\u001b[34mx_train - (5176,) , x_test - (1109,) , x_val - (1110,)\n",
      " y_train - (5176,) , y_test - (1109,) , y_val - (1110,)\u001b[0m\n",
      "\u001b[34mdataset_train length--  5176 , dataset_val length-- 1110,dataset_test length-- 1109\u001b[0m\n",
      "\u001b[34mcuda\u001b[0m\n",
      "\u001b[34mTraining Begins -- \u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.193 algo-1:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.277 algo-1:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.277 algo-1:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.278 algo-1:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.278 algo-1:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.278 algo-1:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.embeddings.word_embeddings.weight count_params:23440896\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.embeddings.position_embeddings.weight count_params:393216\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.embeddings.token_type_embeddings.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.embeddings.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.embeddings.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.342 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.0.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.1.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.343 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.2.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.344 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.3.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.4.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.345 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.5.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.6.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.346 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.7.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.347 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.8.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.9.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.348 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.10.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.query.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.key.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.self.value.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.attention.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.349 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.intermediate.dense.bias count_params:3072\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.weight count_params:2359296\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.output.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.weight count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.encoder.layer.11.output.LayerNorm.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.pooler.dense.weight count_params:589824\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:bert.pooler.dense.bias count_params:768\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:classifier.weight count_params:1536\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:591] name:classifier.bias count_params:2\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:593] Total Trainable Params: 109483778\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:425] Monitoring the collections: losses\u001b[0m\n",
      "\u001b[34m[2022-11-27 11:30:00.350 algo-1:27 INFO hook.py:488] Hook is writing from the hook with pid: 27\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mEpoch 1\u001b[0m\n",
      "\u001b[34mTraining loss: 0.46980270395599527\u001b[0m\n",
      "\u001b[34mValidation loss: 0.44482530932203473\u001b[0m\n",
      "\u001b[34mValidation AUC: 0.883191976480995\u001b[0m\n",
      "\u001b[34mEpoch 2\u001b[0m\n",
      "\u001b[34mTraining loss: 0.3967237152804087\u001b[0m\n",
      "\u001b[34mValidation loss: 0.4587415420322967\u001b[0m\n",
      "\u001b[34mValidation AUC: 0.882174077061165\u001b[0m\n",
      "\n",
      "2022-11-27 11:38:51 Uploading - Uploading generated training model\u001b[34m[0.16690026 0.58673298 0.49015713 ... 0.79235131 0.2160888  0.28396618]\n",
      " Test AUC -- 0.8650594610770652\u001b[0m\n",
      "\u001b[34mSaving model to -- /opt/ml/model\u001b[0m\n",
      "\u001b[34mSaving tokenizer to -- /opt/ml/model\u001b[0m\n",
      "\u001b[34mTraining Finished -- \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 570/570 [00:00<00:00, 689kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]#015Downloading:   2%|▏         | 4.10k/232k [00:00<00:10, 22.1kB/s]#015Downloading:  16%|█▋        | 37.9k/232k [00:00<00:06, 30.0kB/s]#015Downloading:  42%|████▏     | 98.3k/232k [00:00<00:03, 41.2kB/s]#015Downloading:  88%|████████▊ | 205k/232k [00:00<00:00, 57.1kB/s] #015Downloading: 100%|██████████| 232k/232k [00:00<00:00, 310kB/s] \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/466k [00:00<?, ?B/s]#015Downloading:   1%|          | 4.10k/466k [00:00<00:20, 22.1kB/s]#015Downloading:   9%|▉         | 42.0k/466k [00:00<00:14, 30.2kB/s]#015Downloading:  21%|██        | 98.3k/466k [00:00<00:08, 41.3kB/s]#015Downloading:  45%|████▍     | 209k/466k [00:00<00:04, 57.4kB/s] #015Downloading:  94%|█████████▍| 438k/466k [00:00<00:00, 80.3kB/s]#015Downloading: 100%|██████████| 466k/466k [00:00<00:00, 500kB/s] \u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 28.0/28.0 [00:00<00:00, 37.2kB/s]\u001b[0m\n",
      "\u001b[34mTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\u001b[0m\n",
      "\u001b[34m/opt/conda/lib/python3.6/site-packages/transformers/tokenization_utils_base.py:2110: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]#015Downloading:   1%|          | 2.96M/440M [00:00<00:14, 29.5MB/s]#015Downloading:   2%|▏         | 9.83M/440M [00:00<00:12, 35.6MB/s]#015Downloading:   4%|▍         | 18.4M/440M [00:00<00:09, 43.2MB/s]#015Downloading:   6%|▌         | 27.2M/440M [00:00<00:08, 51.0MB/s]#015Downloading:   8%|▊         | 35.9M/440M [00:00<00:06, 58.3MB/s]#015Downloading:  10%|█         | 44.7M/440M [00:00<00:06, 64.7MB/s]#015Downloading:  12%|█▏        | 53.5M/440M [00:00<00:05, 70.4MB/s]#015Downloading:  14%|█▍        | 62.3M/440M [00:00<00:05, 74.9MB/s]#015Downloading:  16%|█▌        | 71.2M/440M [00:00<00:04, 78.5MB/s]#015Downloading:  18%|█▊        | 80.0M/440M [00:01<00:04, 81.3MB/s]#015Downloading:  20%|██        | 88.9M/440M [00:01<00:04, 83.3MB/s]#015Downloading:  22%|██▏       | 97.7M/440M [00:01<00:04, 84.6MB/s]#015Downloading:  24%|██▍       | 106M/440M [00:01<00:03, 85.7MB/s] #015Downloading:  26%|██▌       | 115M/440M [00:01<00:03, 86.5MB/s]#015Downloading:  28%|██▊       | 124M/440M [00:01<00:03, 86.9MB/s]#015Downloading:  30%|███       | 133M/440M [00:01<00:03, 87.3MB/s]#015Downloading:  32%|███▏      | 142M/440M [00:01<00:03, 87.6MB/s]#015Downloading:  34%|███▍      | 151M/440M [00:01<00:03, 87.5MB/s]#015Downloading:  36%|███▌      | 159M/440M [00:01<00:03, 87.6MB/s]#015Downloading:  38%|███▊      | 168M/440M [00:02<00:03, 88.0MB/s]#015Downloading:  40%|████      | 177M/440M [00:02<00:02, 88.2MB/s]#015Downloading:  42%|████▏     | 186M/440M [00:02<00:02, 88.3MB/s]#015Downloading:  44%|████▍     | 195M/440M [00:02<00:02, 88.3MB/s]#015Downloading:  46%|████▌     | 204M/440M [00:02<00:02, 88.5MB/s]#015Downloading:  48%|████▊     | 213M/440M [00:02<00:02, 88.3MB/s]#015Downloading:  50%|█████     | 221M/440M [00:02<00:02, 88.2MB/s]#015Downloading:  52%|█████▏    | 230M/440M [00:02<00:02, 88.3MB/s]#015Downloading:  54%|█████▍    | 239M/440M [00:02<00:02, 88.2MB/s]#015Downloading:  56%|█████▋    | 248M/440M [00:02<00:02, 87.7MB/s]#015Downloading:  58%|█████▊    | 257M/440M [00:03<00:02, 87.9MB/s]#015Downloading:  60%|██████    | 266M/440M [00:03<00:01, 88.0MB/s]#015Downloading:  62%|██████▏   | 274M/440M [00:03<00:01, 88.0MB/s]#015Downloading:  64%|██████▍   | 283M/440M [00:03<00:01, 88.1MB/s]#015Downloading:  66%|██████▋   | 292M/440M [00:03<00:01, 88.2MB/s]#015Downloading:  68%|██████▊   | 301M/440M [00:03<00:01, 88.3MB/s]#015Downloading:  70%|███████   | 310M/440M [00:03<00:01, 88.2MB/s]#015Downloading:  72%|███████▏  | 319M/440M [00:03<00:01, 88.3MB/s]#015Downloading:  74%|███████▍  | 327M/440M [00:03<00:01, 88.4MB/s]#015Downloading:  76%|███████▋  | 336M/440M [00:03<00:01, 88.2MB/s]#015Downloading:  78%|███████▊  | 345M/440M [00:04<00:01, 88.3MB/s]#015Downloading:  80%|████████  | 354M/440M [00:04<00:00, 88.3MB/s]#015Downloading:  82%|████████▏ | 363M/440M [00:04<00:00, 88.0MB/s]#015Downloading:  84%|████████▍ | 372M/440M [00:04<00:00, 88.1MB/s]#015Downloading:  86%|████████▋ | 380M/440M [00:04<00:00, 88.3MB/s]#015Downloading:  88%|████████▊ | 389M/440M [00:04<00:00, 88.3MB/s]#015Downloading:  90%|█████████ | 398M/440M [00:04<00:00, 88.3MB/s]#015Downloading:  92%|█████████▏| 407M/440M [00:04<00:00, 88.2MB/s]#015Downloading:  94%|█████████▍| 416M/440M [00:04<00:00, 88.1MB/s]#015Downloading:  96%|█████████▋| 425M/440M [00:04<00:00, 88.0MB/s]#015Downloading:  98%|█████████▊| 433M/440M [00:05<00:00, 87.6MB/s]#015Downloading: 100%|██████████| 440M/440M [00:05<00:00, 86.5MB/s]\u001b[0m\n",
      "\u001b[34mSome weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m2022-11-27 11:38:43,923 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-11-27 11:39:52 Completed - Training job completed\n",
      "ProfilerReport-1669548346: IssuesFound\n",
      "Training seconds: 623\n",
      "Billable seconds: 623\n"
     ]
    }
   ],
   "source": [
    "local_script_location = \"../src\"\n",
    "hub = {\n",
    "  'HF_TASK':'text-classification'     ## NLP task you want to use for predictions\n",
    "}\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        source_dir=local_script_location,\n",
    "        env=hub, \n",
    "        instance_type='ml.g4dn.12xlarge', \n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.6',\n",
    "        pytorch_version='1.7',\n",
    "        py_version='py36',\n",
    "        hyperparameters = hyperparameters\n",
    ")\n",
    "huggingface_estimator.fit(inputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79ba229d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----!"
     ]
    }
   ],
   "source": [
    "predictor = huggingface_estimator.deploy(initial_instance_count=1, instance_type=\"ml.c5.xlarge\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
